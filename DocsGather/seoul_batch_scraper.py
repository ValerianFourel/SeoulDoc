#!/usr/bin/env python3
"""
Seoul Medical Facilities Batch Scraper V3
Features:
- Fresh browser for each dong (driver recreated)
- Parallel processing with distributed start points
- Dong considered complete when first keyword CSV has ‚â•40 entries
Structure: district/dong/keyword.json
"""

import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
import pandas as pd
from multiprocessing import Pool
import traceback
import filelock
import csv

from naver_medical_scraper_v6 import NaverMedicalScraperV6

# Seoul administrative dongs data
seoul_administrative_dongs = {
    "Gangnam-gu": [
        "Í∞úÌè¨1Îèô", "Í∞úÌè¨2Îèô", "Í∞úÌè¨3Îèô", "Í∞úÌè¨4Îèô", "ÎÖºÌòÑ1Îèô", "ÎÖºÌòÑ2Îèô",
        "ÎåÄÏπò1Îèô", "ÎåÄÏπò2Îèô", "ÎåÄÏπò4Îèô", "ÎèÑÍ≥°1Îèô", "ÎèÑÍ≥°2Îèô", "ÏÇºÏÑ±1Îèô",
        "ÏÇºÏÑ±2Îèô", "ÏÑ∏Í≥°Îèô", "ÏàòÏÑúÎèô", "Ïã†ÏÇ¨Îèô", "ÏïïÍµ¨Ï†ïÎèô", "Ïó≠ÏÇº1Îèô",
        "Ïó≠ÏÇº2Îèô", "ÏùºÏõê1Îèô", "ÏùºÏõêÎ≥∏Îèô", "Ï≤≠Îã¥Îèô"
    ],
    "Gangdong-gu": [
        "Í∞ïÏùºÎèô", "Í≥†Îçï1Îèô", "Í≥†Îçï2Îèô", "Í∏∏Îèô", "ÎëîÏ¥å1Îèô", "ÎëîÏ¥å2Îèô",
        "Î™ÖÏùº1Îèô", "Î™ÖÏùº2Îèô", "ÏÉÅÏùº1Îèô", "ÏÉÅÏùº2Îèô", "ÏÑ±ÎÇ¥1Îèô", "ÏÑ±ÎÇ¥2Îèô",
        "ÏÑ±ÎÇ¥3Îèô", "ÏïîÏÇ¨1Îèô", "ÏïîÏÇ¨2Îèô", "ÏïîÏÇ¨3Îèô", "Ï≤úÌò∏1Îèô", "Ï≤úÌò∏2Îèô", "Ï≤úÌò∏3Îèô"
    ],
    "Gangbuk-gu": [
        "ÎØ∏ÏïÑÎèô", "Î≤à1Îèô", "Î≤à2Îèô", "Î≤à3Îèô", "ÏÇºÍ∞ÅÏÇ∞Îèô", "ÏÇºÏñëÎèô",
        "ÏÜ°Ï§ëÎèô", "ÏÜ°Ï≤úÎèô", "ÏàòÏú†1Îèô", "ÏàòÏú†2Îèô", "ÏàòÏú†3Îèô", "Ïö∞Ïù¥Îèô", "Ïù∏ÏàòÎèô"
    ],
    "Gangseo-gu": [
        "Í∞ÄÏñë1Îèô", "Í∞ÄÏñë2Îèô", "Í∞ÄÏñë3Îèô", "Í≥µÌï≠Îèô", "Îì±Ï¥å1Îèô", "Îì±Ï¥å2Îèô",
        "Îì±Ï¥å3Îèô", "Î∞úÏÇ∞1Îèô", "Î∞©Ìôî1Îèô", "Î∞©Ìôî2Îèô", "Î∞©Ìôî3Îèô", "ÏóºÏ∞ΩÎèô",
        "Ïö∞Ïû•ÏÇ∞Îèô", "ÌôîÍ≥°1Îèô", "ÌôîÍ≥°2Îèô", "ÌôîÍ≥°3Îèô", "ÌôîÍ≥°4Îèô", "ÌôîÍ≥°6Îèô",
        "ÌôîÍ≥°8Îèô", "ÌôîÍ≥°Î≥∏Îèô"
    ],
    "Gwanak-gu": [
        "ÎÇôÏÑ±ÎåÄÎèô", "ÎÇúÍ≥°Îèô", "ÎÇúÌñ•Îèô", "ÎÇ®ÌòÑÎèô", "ÎåÄÌïôÎèô", "ÎØ∏ÏÑ±Îèô",
        "Î≥¥ÎùºÎß§Îèô", "ÏÇºÏÑ±Îèô", "ÏÑúÎ¶ºÎèô", "ÏÑúÏõêÎèô", "ÏÑ±ÌòÑÎèô", "Ïã†Î¶ºÎèô",
        "Ïã†ÏÇ¨Îèô", "Ïã†ÏõêÎèô", "ÏùÄÏ≤úÎèô", "Ïù∏ÌóåÎèô", "Ï°∞ÏõêÎèô", "Ï§ëÏïôÎèô",
        "Ï≤≠Î£°Îèô", "Ï≤≠Î¶ºÎèô", "ÌñâÏö¥Îèô"
    ],
    "Gwangjin-gu": [
        "Í¥ëÏû•Îèô", "Íµ¨Ïùò1Îèô", "Íµ¨Ïùò2Îèô", "Íµ¨Ïùò3Îèô", "Íµ∞ÏûêÎèô", "Îä•Îèô",
        "ÏûêÏñë1Îèô", "ÏûêÏñë2Îèô", "ÏûêÏñë3Îèô", "ÏûêÏñë4Îèô", "Ï§ëÍ≥°1Îèô", "Ï§ëÍ≥°2Îèô",
        "Ï§ëÍ≥°3Îèô", "Ï§ëÍ≥°4Îèô", "ÌôîÏñëÎèô"
    ],
    "Guro-gu": [
        "Í∞ÄÎ¶¨Î¥âÎèô", "Í∞úÎ¥â1Îèô", "Í∞úÎ¥â2Îèô", "Í∞úÎ¥â3Îèô", "Í≥†Ï≤ô1Îèô", "Í≥†Ï≤ô2Îèô",
        "Íµ¨Î°ú1Îèô", "Íµ¨Î°ú2Îèô", "Íµ¨Î°ú3Îèô", "Íµ¨Î°ú4Îèô", "Íµ¨Î°ú5Îèô", "ÏàòÍ∂ÅÎèô",
        "Ïã†ÎèÑÎ¶ºÎèô", "Ïò§Î•ò1Îèô", "Ïò§Î•ò2Îèô", "Ìï≠Îèô"
    ],
    "Geumcheon-gu": [
        "Í∞ÄÏÇ∞Îèô", "ÎèÖÏÇ∞1Îèô", "ÎèÖÏÇ∞2Îèô", "ÎèÖÏÇ∞3Îèô", "ÎèÖÏÇ∞4Îèô",
        "ÏãúÌù•1Îèô", "ÏãúÌù•2Îèô", "ÏãúÌù•3Îèô", "ÏãúÌù•4Îèô", "ÏãúÌù•5Îèô"
    ],
    "Nowon-gu": [
        "Í≥µÎ¶â1Îèô", "Í≥µÎ¶â2Îèô", "ÏÉÅÍ≥Ñ1Îèô", "ÏÉÅÍ≥Ñ2Îèô", "ÏÉÅÍ≥Ñ3.4Îèô", "ÏÉÅÍ≥Ñ5Îèô",
        "ÏÉÅÍ≥Ñ6.7Îèô", "ÏÉÅÍ≥Ñ8Îèô", "ÏÉÅÍ≥Ñ9Îèô", "ÏÉÅÍ≥Ñ10Îèô", "ÏõîÍ≥Ñ1Îèô", "ÏõîÍ≥Ñ2Îèô",
        "ÏõîÍ≥Ñ3Îèô", "Ï§ëÍ≥ÑÎ≥∏Îèô", "Ï§ëÍ≥Ñ1Îèô", "Ï§ëÍ≥Ñ2.3Îèô", "Ï§ëÍ≥Ñ4Îèô", "ÌïòÍ≥Ñ1Îèô", "ÌïòÍ≥Ñ2Îèô"
    ],
    "Dobong-gu": [
        "ÎèÑÎ¥â1Îèô", "ÎèÑÎ¥â2Îèô", "Î∞©Ìïô1Îèô", "Î∞©Ìïô2Îèô", "Î∞©Ìïô3Îèô",
        "ÏåçÎ¨∏1Îèô", "ÏåçÎ¨∏2Îèô", "ÏåçÎ¨∏3Îèô", "ÏåçÎ¨∏4Îèô",
        "Ï∞Ω1Îèô", "Ï∞Ω2Îèô", "Ï∞Ω3Îèô", "Ï∞Ω4Îèô", "Ï∞Ω5Îèô"
    ],
    "Dongdaemun-gu": [
        "ÎãµÏã≠Î¶¨1Îèô", "ÎãµÏã≠Î¶¨2Îèô", "Ïö©Ïã†Îèô", "Ïù¥Î¨∏1Îèô", "Ïù¥Î¨∏2Îèô",
        "Ïû•Ïïà1Îèô", "Ïû•Ïïà2Îèô", "Ï†ÑÎÜç1Îèô", "Ï†ÑÎÜç2Îèô", "Ï†úÍ∏∞Îèô",
        "Ï≤≠ÎüâÎ¶¨Îèô", "ÌöåÍ∏∞Îèô", "ÌúòÍ≤Ω1Îèô", "ÌúòÍ≤Ω2Îèô"
    ],
    "Dongjak-gu": [
        "ÎÖ∏ÎüâÏßÑ1Îèô", "ÎÖ∏ÎüâÏßÑ2Îèô", "ÎåÄÎ∞©Îèô", "ÏÇ¨Îãπ1Îèô", "ÏÇ¨Îãπ2Îèô",
        "ÏÇ¨Îãπ3Îèô", "ÏÇ¨Îãπ4Îèô", "ÏÇ¨Îãπ5Îèô", "ÏÉÅÎèÑ1Îèô", "ÏÉÅÎèÑ2Îèô",
        "ÏÉÅÎèÑ3Îèô", "ÏÉÅÎèÑ4Îèô", "Ïã†ÎåÄÎ∞©1Îèô", "Ïã†ÎåÄÎ∞©2Îèô", "ÌùëÏÑùÎèô"
    ],
    "Mapo-gu": [
        "Í≥µÎçïÎèô", "ÎåÄÌù•Îèô", "ÎèÑÌôîÎèô", "ÎßùÏõê1Îèô", "ÎßùÏõê2Îèô", "ÏÉÅÏïîÎèô",
        "ÏÑúÍ∞ïÎèô", "ÏÑúÍµêÎèô", "ÏÑ±ÏÇ∞1Îèô", "ÏÑ±ÏÇ∞2Îèô", "Ïã†ÏàòÎèô", "ÏïÑÌòÑÎèô",
        "Ïó∞ÎÇ®Îèô", "ÏóºÎ¶¨Îèô", "Ïö©Í∞ïÎèô", "Ìï©Ï†ïÎèô"
    ],
    "Seodaemun-gu": [
        "ÎÇ®Í∞ÄÏ¢å1Îèô", "ÎÇ®Í∞ÄÏ¢å2Îèô", "Î∂ÅÍ∞ÄÏ¢å1Îèô", "Î∂ÅÍ∞ÄÏ¢å2Îèô", "Î∂ÅÏïÑÌòÑÎèô",
        "Ïã†Ï¥åÎèô", "Ïó∞Ìù¨Îèô", "Ï≤úÏó∞Îèô", "Ï∂©ÌòÑÎèô", "ÌôçÏùÄ1Îèô", "ÌôçÏùÄ2Îèô",
        "ÌôçÏ†ú1Îèô", "ÌôçÏ†ú2Îèô", "ÌôçÏ†ú3Îèô"
    ],
    "Seocho-gu": [
        "ÎÇ¥Í≥°Îèô", "Î∞òÌè¨Î≥∏Îèô", "Î∞òÌè¨1Îèô", "Î∞òÌè¨2Îèô", "Î∞òÌè¨3Îèô", "Î∞òÌè¨4Îèô",
        "Î∞©Î∞∞Î≥∏Îèô", "Î∞©Î∞∞1Îèô", "Î∞©Î∞∞2Îèô", "Î∞©Î∞∞3Îèô", "Î∞©Î∞∞4Îèô",
        "ÏÑúÏ¥à1Îèô", "ÏÑúÏ¥à2Îèô", "ÏÑúÏ¥à3Îèô", "ÏÑúÏ¥à4Îèô", "ÏñëÏû¨1Îèô", "ÏñëÏû¨2Îèô", "Ïû†ÏõêÎèô"
    ],
    "Seongdong-gu": [
        "Í∏àÌò∏1Í∞ÄÎèô", "Í∏àÌò∏2.3Í∞ÄÎèô", "Í∏àÌò∏4Í∞ÄÎèô", "ÎßàÏû•Îèô", "ÏÇ¨Í∑ºÎèô",
        "ÏÑ±Ïàò1Í∞Ä1Îèô", "ÏÑ±Ïàò1Í∞Ä2Îèô", "ÏÑ±Ïàò2Í∞Ä1Îèô", "ÏÑ±Ïàò2Í∞Ä3Îèô", "ÏÜ°Ï†ïÎèô",
        "Ïò•ÏàòÎèô", "ÏôïÏã≠Î¶¨ÎèÑÏÑ†Îèô", "ÏôïÏã≠Î¶¨2Îèô", "Ïö©ÎãµÎèô", "ÏùëÎ¥âÎèô",
        "ÌñâÎãπ1Îèô", "ÌñâÎãπ2Îèô"
    ],
    "Seongbuk-gu": [
        "Í∏∏Ïùå1Îèô", "Í∏∏Ïùå2Îèô", "ÎèàÏïî1Îèô", "ÎèàÏïî2Îèô", "ÎèôÏÑ†Îèô", "Î≥¥Î¨∏Îèô",
        "ÏÇºÏÑ†Îèô", "ÏÑùÍ¥ÄÎèô", "ÏÑ±Î∂ÅÎèô", "ÏïàÏïîÎèô", "ÏõîÍ≥°1Îèô", "ÏõîÍ≥°2Îèô",
        "Ïû•ÏúÑ1Îèô", "Ïû•ÏúÑ2Îèô", "Ïû•ÏúÑ3Îèô", "Ï†ïÎ¶â1Îèô", "Ï†ïÎ¶â2Îèô",
        "Ï†ïÎ¶â3Îèô", "Ï†ïÎ¶â4Îèô", "Ï¢ÖÏïîÎèô"
    ],
    "Songpa-gu": [
        "Í∞ÄÎùΩÎ≥∏Îèô", "Í∞ÄÎùΩ1Îèô", "Í∞ÄÎùΩ2Îèô", "Í±∞Ïó¨1Îèô", "Í±∞Ïó¨2Îèô",
        "ÎßàÏ≤ú1Îèô", "ÎßàÏ≤ú2Îèô", "Î¨∏Ï†ï1Îèô", "Î¨∏Ï†ï2Îèô", "Î∞©Ïù¥1Îèô", "Î∞©Ïù¥2Îèô",
        "ÏÇºÏ†ÑÎèô", "ÏÑùÏ¥åÎèô", "ÏÜ°Ìåå1Îèô", "ÏÜ°Ìåå2Îèô", "Ïò§Í∏àÎèô", "Ïò§Î•úÎèô",
        "ÏúÑÎ°ÄÎèô", "Ïû†Ïã§Î≥∏Îèô", "Ïû†Ïã§2Îèô", "Ïû†Ïã§3Îèô", "Ïû†Ïã§4Îèô",
        "Ïû†Ïã§6Îèô", "Ïû†Ïã§7Îèô", "Ïû•ÏßÄÎèô", "ÌíçÎÇ©1Îèô", "ÌíçÎÇ©2Îèô"
    ],
    "Yangcheon-gu": [
        "Î™©1Îèô", "Î™©2Îèô", "Î™©3Îèô", "Î™©4Îèô", "Î™©5Îèô",
        "Ïã†Ïõî1Îèô", "Ïã†Ïõî2Îèô", "Ïã†Ïõî3Îèô", "Ïã†Ïõî4Îèô", "Ïã†Ïõî5Îèô", "Ïã†Ïõî6Îèô", "Ïã†Ïõî7Îèô",
        "Ïã†Ï†ï1Îèô", "Ïã†Ï†ï2Îèô", "Ïã†Ï†ï3Îèô", "Ïã†Ï†ï4Îèô", "Ïã†Ï†ï6Îèô", "Ïã†Ï†ï7Îèô"
    ],
    "Yeongdeungpo-gu": [
        "ÎãπÏÇ∞1Îèô", "ÎãπÏÇ∞2Îèô", "ÎåÄÎ¶º1Îèô", "ÎåÄÎ¶º2Îèô", "ÎåÄÎ¶º3Îèô", "ÎèÑÎ¶ºÎèô",
        "Î¨∏ÎûòÎèô", "Ïã†Í∏∏1Îèô", "Ïã†Í∏∏3Îèô", "Ïã†Í∏∏4Îèô", "Ïã†Í∏∏5Îèô", "Ïã†Í∏∏6Îèô", "Ïã†Í∏∏7Îèô",
        "ÏñëÌèâ1Îèô", "ÏñëÌèâ2Îèô", "Ïó¨ÏùòÎèô", "ÏòÅÎì±Ìè¨Î≥∏Îèô", "ÏòÅÎì±Ìè¨Îèô"
    ],
    "Yongsan-gu": [
        "ÎÇ®ÏòÅÎèô", "Î≥¥Í¥ëÎèô", "ÏÑúÎπôÍ≥†Îèô", "Ïö©Î¨∏Îèô", "Ïö©ÏÇ∞2Í∞ÄÎèô",
        "ÏõêÌö®Î°ú1Îèô", "ÏõêÌö®Î°ú2Îèô", "Ïù¥Ï¥å1Îèô", "Ïù¥Ï¥å2Îèô",
        "Ïù¥ÌÉúÏõê1Îèô", "Ïù¥ÌÉúÏõê2Îèô", "Ï≤≠ÌååÎèô", "ÌïúÍ∞ïÎ°úÎèô", "ÌïúÎÇ®Îèô",
        "Ìö®Ï∞ΩÎèô", "ÌõÑÏïîÎèô"
    ],
    "Eunpyeong-gu": [
        "Í∞àÌòÑ1Îèô", "Í∞àÌòÑ2Îèô", "Íµ¨ÏÇ∞Îèô", "ÎÖπÎ≤àÎèô", "ÎåÄÏ°∞Îèô",
        "Î∂àÍ¥ë1Îèô", "Î∂àÍ¥ë2Îèô", "ÏàòÏÉâÎèô", "Ïã†ÏÇ¨1Îèô", "Ïã†ÏÇ¨2Îèô",
        "Ïó≠Ï¥åÎèô", "ÏùëÏïî1Îèô", "ÏùëÏïî2Îèô", "ÏùëÏïî3Îèô", "Ï¶ùÏÇ∞Îèô", "ÏßÑÍ¥ÄÎèô"
    ],
    "Jongno-gu": [
        "Í∞ÄÌöåÎèô", "ÍµêÎÇ®Îèô", "Î¨¥ÏïÖÎèô", "Î∂ÄÏïîÎèô", "ÏÇ¨ÏßÅÎèô", "ÏÇºÏ≤≠Îèô",
        "Ïà≠Ïù∏1Îèô", "Ïà≠Ïù∏2Îèô", "Ïù¥ÌôîÎèô", "Ï¢ÖÎ°ú1.2.3.4Í∞ÄÎèô", "Ï¢ÖÎ°ú5.6Í∞ÄÎèô",
        "Ï∞ΩÏã†1Îèô", "Ï∞ΩÏã†2Îèô", "Ï∞ΩÏã†3Îèô", "Ï≤≠Ïö¥Ìö®ÏûêÎèô", "ÌèâÏ∞ΩÎèô", "ÌòúÌôîÎèô"
    ],
    "Jung-gu": [
        "Í¥ëÌù¨Îèô", "Îã§ÏÇ∞Îèô", "ÎèôÌôîÎèô", "Î™ÖÎèô", "ÏÜåÍ≥µÎèô", "Ïã†ÎãπÎèô", "Ïã†Îãπ5Îèô",
        "ÏïΩÏàòÎèô", "ÏùÑÏßÄÎ°úÎèô", "Ïû•Ï∂©Îèô", "Ï§ëÎ¶ºÎèô", "Ï≤≠Íµ¨Îèô", "ÌïÑÎèô",
        "Ìô©ÌïôÎèô", "ÌöåÌòÑÎèô"
    ],
    "Jungnang-gu": [
        "ÎßùÏö∞Î≥∏Îèô", "ÎßùÏö∞3Îèô", "Î©¥Î™©Î≥∏Îèô", "Î©¥Î™©2Îèô", "Î©¥Î™©3.8Îèô",
        "Î©¥Î™©4Îèô", "Î©¥Î™©5Îèô", "Î©¥Î™©7Îèô", "Î¨µ1Îèô", "Î¨µ2Îèô",
        "ÏÉÅÎ¥â1Îèô", "ÏÉÅÎ¥â2Îèô", "Ïã†ÎÇ¥1Îèô", "Ïã†ÎÇ¥2Îèô", "Ï§ëÌôî1Îèô", "Ï§ëÌôî2Îèô"
    ]
}


def count_csv_rows(csv_path: Path) -> int:
    """Count rows in CSV file (excluding header)"""
    try:
        if not csv_path.exists():
            return 0
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            next(reader, None)  # Skip header
            return sum(1 for row in reader)
    except:
        return 0


def is_dong_complete(output_dir: Path, gu: str, dong: str, first_keyword: str = 'Î≥ëÏõê', min_entries: int = 40) -> bool:
    """
    Check if dong is complete by checking first keyword CSV has >= min_entries
    
    Args:
        output_dir: Base output directory
        gu: District name
        dong: Dong name
        first_keyword: First keyword to check (default: Î≥ëÏõê)
        min_entries: Minimum entries required (default: 40)
    
    Returns:
        True if CSV exists with >= min_entries rows
    """
    csv_path = output_dir / gu / dong / f"{first_keyword}.csv"
    row_count = count_csv_rows(csv_path)
    return row_count >= min_entries


def scrape_single_dong(task: Dict, output_dir: str, headless: bool, max_pages: int, min_entries: int = 40) -> Dict:
    """
    Scrape a single dong with all keywords
    Creates fresh browser for this dong only
    Dong is complete when first keyword CSV has >= min_entries
    
    Args:
        task: Dict with 'gu', 'dong', 'keywords'
        output_dir: Base output directory
        headless: Run headless
        max_pages: Max pages per keyword
        min_entries: Minimum entries to consider complete
    
    Returns:
        Dict with results summary
    """
    gu = task['gu']
    dong = task['dong']
    keywords = task['keywords']
    
    output_path = Path(output_dir)
    results_summary = {
        'gu': gu,
        'dong': dong,
        'completed_keywords': [],
        'failed_keywords': [],
        'total_facilities': 0,
        'start_time': datetime.now().isoformat(),
        'end_time': None,
        'error': None
    }
    
    print(f"\n{'='*70}")
    print(f"üîÑ Starting NEW browser for: {gu} > {dong}")
    print(f"   Keywords: {', '.join(keywords)}")
    print(f"{'='*70}")
    
    # Create fresh browser for this dong
    scraper = None
    try:
        scraper = NaverMedicalScraperV6(headless=headless)
        
        for keyword in keywords:
            print(f"\n{'‚îÄ'*70}")
            print(f"üìç {gu} > {dong} > {keyword}")
            print(f"{'‚îÄ'*70}")
            
            try:
                # Scrape this keyword
                results = scraper.scrape_location(
                    query=keyword,
                    location=dong,
                    max_pages=max_pages
                )
                
                # Save results
                district_dir = output_path / gu
                dong_dir = district_dir / dong
                dong_dir.mkdir(parents=True, exist_ok=True)
                
                json_path = dong_dir / f"{keyword}.json"
                csv_path = dong_dir / f"{keyword}.csv"
                
                if results:
                    # Save JSON
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    
                    # Save CSV
                    try:
                        scraper.save_to_csv(results, str(csv_path))
                        
                        # Check if we have enough entries for first keyword
                        if keyword == keywords[0]:
                            row_count = count_csv_rows(csv_path)
                            print(f"    üìä First keyword CSV: {row_count} entries")
                            if row_count >= min_entries:
                                print(f"    ‚úÖ Reached minimum {min_entries} entries - dong considered complete!")
                    except Exception as csv_err:
                        print(f"    ‚ö†Ô∏è  CSV save warning: {csv_err}")
                    
                    print(f"\n‚úÖ Saved {len(results)} results for {keyword}")
                    results_summary['total_facilities'] += len(results)
                else:
                    # Save empty file
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump([], f)
                    print(f"\n‚ö†Ô∏è  No results for {keyword}")
                
                results_summary['completed_keywords'].append(keyword)
                
            except Exception as kw_error:
                print(f"\n‚ùå Error for {keyword}: {kw_error}")
                print(traceback.format_exc())
                results_summary['failed_keywords'].append(keyword)
                
                # Save empty file to mark as attempted
                try:
                    dong_dir.mkdir(parents=True, exist_ok=True)
                    json_path = dong_dir / f"{keyword}.json"
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump([], f)
                except:
                    pass
        
        results_summary['end_time'] = datetime.now().isoformat()
        print(f"\n‚úÖ Completed {gu} > {dong}")
        print(f"   Total facilities: {results_summary['total_facilities']}")
        
    except Exception as e:
        results_summary['error'] = str(e)
        results_summary['end_time'] = datetime.now().isoformat()
        print(f"\n‚ùå Fatal error for {gu} > {dong}: {e}")
        print(traceback.format_exc())
    
    finally:
        # ALWAYS close browser for this dong
        if scraper:
            try:
                scraper.close()
                print(f"üîö Closed browser for {gu} > {dong}")
            except:
                pass
    
    return results_summary


class SeoulMedicalBatchScraperV3:
    """
    Batch scraper with parallel processing and fresh browsers per dong
    """
    
    def __init__(self, output_dir: str = 'seoul_medical_data', min_entries: int = 40):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.progress_file = self.output_dir / 'progress.json'
        self.progress_lock_file = self.output_dir / 'progress.json.lock'
        
        self.keywords = ['Î≥ëÏõê', 'ÏùòÏõê', 'ÌÅ¥Î¶¨Îãâ']
        self.min_entries = min_entries
        
        self.total_dongs = sum(len(dongs) for dongs in seoul_administrative_dongs.values())
        self.total_tasks = self.total_dongs * len(self.keywords)
        
        print(f"\n{'='*60}")
        print(f"Seoul Medical Facilities Batch Scraper V3")
        print(f"{'='*60}")
        print(f"Districts: {len(seoul_administrative_dongs)}")
        print(f"Dongs: {self.total_dongs}")
        print(f"Keywords: {', '.join(self.keywords)}")
        print(f"Total tasks: {self.total_tasks}")
        print(f"Completion criteria: First keyword CSV ‚â• {self.min_entries} entries")
        print(f"‚ú® Features: Fresh browser per dong, parallel processing")
        print(f"{'='*60}\n")
    
    def _load_progress(self) -> Dict:
        """Load progress with file locking"""
        lock = filelock.FileLock(str(self.progress_lock_file))
        
        try:
            with lock.acquire(timeout=10):
                if self.progress_file.exists():
                    with open(self.progress_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
        except:
            pass
        
        return {
            'completed_dongs': [],
            'statistics': {
                'total_dongs_completed': 0,
                'total_facilities': 0,
                'by_keyword': {}
            },
            'start_time': datetime.now().isoformat()
        }
    
    def _save_progress(self, dong_summary: Dict):
        """Save progress with file locking"""
        lock = filelock.FileLock(str(self.progress_lock_file))
        
        try:
            with lock.acquire(timeout=10):
                # Load current progress
                progress_data = self._load_progress()
                
                # Add this dong
                dong_key = f"{dong_summary['gu']}_{dong_summary['dong']}"
                if dong_key not in progress_data['completed_dongs']:
                    progress_data['completed_dongs'].append(dong_key)
                
                # Update statistics
                progress_data['statistics']['total_dongs_completed'] = len(progress_data['completed_dongs'])
                progress_data['statistics']['total_facilities'] = \
                    progress_data['statistics'].get('total_facilities', 0) + dong_summary['total_facilities']
                
                for keyword in dong_summary['completed_keywords']:
                    if keyword not in progress_data['statistics']['by_keyword']:
                        progress_data['statistics']['by_keyword'][keyword] = 0
                    progress_data['statistics']['by_keyword'][keyword] += \
                        dong_summary['total_facilities'] // max(len(dong_summary['completed_keywords']), 1)
                
                progress_data['last_updated'] = datetime.now().isoformat()
                progress_data['completion_percentage'] = \
                    (len(progress_data['completed_dongs']) / self.total_dongs * 100)
                
                # Save
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Progress save error: {e}")
    
    def _is_dong_completed(self, gu: str, dong: str) -> bool:
        """
        Check if dong is completed by checking:
        1. Progress file (tracked completion)
        2. First keyword CSV exists with >= min_entries rows
        """
        # Check progress file first
        progress_data = self._load_progress()
        dong_key = f"{gu}_{dong}"
        if dong_key in progress_data.get('completed_dongs', []):
            return True
        
        # Check if CSV exists with enough entries
        return is_dong_complete(self.output_dir, gu, dong, self.keywords[0], self.min_entries)
    
    def show_progress(self):
        """Display progress"""
        progress_data = self._load_progress()
        
        completed = len(progress_data.get('completed_dongs', []))
        percentage = completed / self.total_dongs * 100
        
        print(f"\n{'='*60}")
        print(f"PROGRESS STATUS")
        print(f"{'='*60}")
        print(f"Completed dongs: {completed}/{self.total_dongs} ({percentage:.1f}%)")
        print(f"Completion criteria: First keyword CSV ‚â• {self.min_entries} entries")
        
        stats = progress_data.get('statistics', {})
        if stats:
            print(f"\nTotal facilities: {stats.get('total_facilities', 0):,}")
            
            by_keyword = stats.get('by_keyword', {})
            if by_keyword:
                print(f"\nBy keyword:")
                for keyword, count in by_keyword.items():
                    print(f"  {keyword}: {count:,}")
        
        print(f"{'='*60}\n")
    
    def scrape_all_seoul(self, headless: bool = True, max_pages: int = 10, 
                        first_page_only: bool = False, workers: int = 1):
        """
        Scrape all Seoul with parallel processing
        Workers start at different points for better distribution
        
        Args:
            headless: Run headless
            max_pages: Max pages per location
            first_page_only: Only first page
            workers: Number of parallel workers (1 = sequential)
        """
        if first_page_only:
            max_pages = 1
            print(f"\n‚ö° FIRST PAGE ONLY MODE")
        
        if workers > 1:
            print(f"\nüöÄ PARALLEL MODE: {workers} workers")
            print(f"   Workers will start at different points for even distribution")
        
        # Collect pending dongs
        pending_dongs = []
        
        for gu, dongs in seoul_administrative_dongs.items():
            for dong in dongs:
                if not self._is_dong_completed(gu, dong):
                    pending_dongs.append({
                        'gu': gu,
                        'dong': dong,
                        'keywords': self.keywords
                    })
        
        print(f"\n{'='*60}")
        print(f"TASK SUMMARY")
        print(f"{'='*60}")
        print(f"Total dongs: {self.total_dongs}")
        print(f"Completed: {self.total_dongs - len(pending_dongs)}")
        print(f"Pending: {len(pending_dongs)}")
        print(f"Max pages: {max_pages}")
        print(f"Workers: {workers}")
        print(f"{'='*60}\n")
        
        if not pending_dongs:
            print("üéâ All dongs completed!")
            return
        
        # Distribute work across workers
        if workers > 1:
            # Split pending_dongs into chunks for each worker
            worker_chunks = [[] for _ in range(workers)]
            for idx, dong in enumerate(pending_dongs):
                worker_chunks[idx % workers].append(dong)
            
            print(f"üìä Work distribution:")
            for i, chunk in enumerate(worker_chunks):
                if chunk:
                    first_dong = f"{chunk[0]['gu']}/{chunk[0]['dong']}"
                    print(f"   Worker {i+1}: {len(chunk)} dongs (starting: {first_dong})")
            print()
        
        # Process dongs
        if workers == 1:
            # Sequential
            for idx, task in enumerate(pending_dongs, 1):
                print(f"\n{'#'*70}")
                print(f"Dong {idx}/{len(pending_dongs)}")
                print(f"{'#'*70}")
                
                summary = scrape_single_dong(task, str(self.output_dir), headless, max_pages, self.min_entries)
                self._save_progress(summary)
                
                completed = self.total_dongs - len(pending_dongs) + idx
                pct = completed / self.total_dongs * 100
                print(f"\nüìä Overall: {completed}/{self.total_dongs} ({pct:.1f}%)")
        
        else:
            # Parallel with distributed start points
            print(f"üöÄ Starting {workers} parallel workers at different points...\n")
            
            from functools import partial
            scrape_func = partial(
                scrape_single_dong,
                output_dir=str(self.output_dir),
                headless=headless,
                max_pages=max_pages,
                min_entries=self.min_entries
            )
            
            with Pool(processes=workers) as pool:
                for idx, summary in enumerate(pool.imap_unordered(scrape_func, pending_dongs), 1):
                    self._save_progress(summary)
                    
                    completed = self.total_dongs - len(pending_dongs) + idx
                    pct = completed / self.total_dongs * 100
                    print(f"\nüìä [{summary['gu']}/{summary['dong']}] Overall: {completed}/{self.total_dongs} ({pct:.1f}%)")
        
        print(f"\n{'='*60}")
        print(f"‚úÖ ALL DONGS COMPLETED!")
        print(f"{'='*60}\n")
    
    def get_statistics(self):
        """Show statistics"""
        self.show_progress()
        
        total_json = len(list(self.output_dir.rglob('*.json'))) - 1
        total_csv = len(list(self.output_dir.rglob('*.csv')))
        
        print(f"\nFiles: {total_json} JSON, {total_csv} CSV")
        
        # Check completion status
        print(f"\nCompletion check (first keyword CSV ‚â• {self.min_entries} entries):")
        complete_count = 0
        for gu, dongs in seoul_administrative_dongs.items():
            for dong in dongs:
                if is_dong_complete(self.output_dir, gu, dong, self.keywords[0], self.min_entries):
                    complete_count += 1
        
        print(f"  Dongs with ‚â•{self.min_entries} entries: {complete_count}/{self.total_dongs}")
    
    def merge_results(self):
        """Merge all results"""
        print(f"\nüìÅ Merging results...")
        
        all_json_files = [f for f in self.output_dir.rglob('*.json') 
                          if f.name not in ['progress.json']]
        
        if not all_json_files:
            print("No files to merge.")
            return
        
        all_data = []
        for json_file in all_json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data:
                        relative_path = json_file.relative_to(self.output_dir)
                        for item in data:
                            item['file_district'] = relative_path.parts[0]
                            item['file_dong'] = relative_path.parts[1]
                            item['file_keyword'] = relative_path.stem
                        all_data.extend(data)
            except:
                pass
        
        if not all_data:
            print("No data to merge.")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        merged_json = self.output_dir / f'_merged_all_{timestamp}.json'
        with open(merged_json, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        merged_csv = self.output_dir / f'_merged_all_{timestamp}.csv'
        df = pd.DataFrame(all_data)
        df.to_csv(merged_csv, index=False, encoding='utf-8-sig')
        
        print(f"\n‚úÖ Merged!")
        print(f"   Rows: {len(all_data):,}")
        print(f"   Unique: {df['place_id'].nunique():,}")


def main():
    """Main execution"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Seoul Medical Batch Scraper V3 - Fresh browsers + Parallel + Smart completion',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Sequential (1 dong at a time)
  python seoul_batch_scraper.py --headless
  
  # Parallel (4 dongs at once, start at different points)
  python seoul_batch_scraper.py --headless --workers 4
  
  # Fast parallel mode
  python seoul_batch_scraper.py --headless --first-page-only --workers 4
  
  # Custom minimum entries
  python seoul_batch_scraper.py --headless --min-entries 30
  
  # Progress
  python seoul_batch_scraper.py --progress
  
  # Merge
  python seoul_batch_scraper.py --merge
        """
    )
    
    parser.add_argument('--output-dir', default='seoul_medical_data')
    parser.add_argument('--headless', action='store_true')
    parser.add_argument('--max-pages', type=int, default=10)
    parser.add_argument('--first-page-only', action='store_true')
    parser.add_argument('--workers', type=int, default=1,
                       help='Number of parallel workers (1-8)')
    parser.add_argument('--min-entries', type=int, default=40,
                       help='Minimum CSV entries to consider dong complete (default: 40)')
    parser.add_argument('--progress', action='store_true')
    parser.add_argument('--stats', action='store_true')
    parser.add_argument('--merge', action='store_true')
    parser.add_argument('--test', action='store_true')
    
    args = parser.parse_args()
    
    scraper = SeoulMedicalBatchScraperV3(
        output_dir=args.output_dir,
        min_entries=args.min_entries
    )
    
    if args.progress or args.stats:
        scraper.show_progress()
        if args.stats:
            scraper.get_statistics()
        return
    
    if args.merge:
        scraper.merge_results()
        return
    
    if args.test:
        print("\nüß™ TEST MODE")
        task = {
            'gu': 'Gangnam-gu',
            'dong': 'Í∞úÌè¨1Îèô',
            'keywords': ['Î≥ëÏõê']
        }
        summary = scrape_single_dong(task, args.output_dir, False, 2, args.min_entries)
        print(f"\n‚úÖ Test complete: {summary}")
        return
    
    # Run scraping
    scraper.scrape_all_seoul(
        headless=args.headless,
        max_pages=args.max_pages,
        first_page_only=args.first_page_only,
        workers=min(args.workers, 8)  # Max 8 workers
    )


if __name__ == "__main__":
    main()