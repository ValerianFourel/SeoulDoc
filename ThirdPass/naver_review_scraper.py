#!/usr/bin/env python3
"""
Naver Maps Review Scraper
Scrapes reviews from Naver Maps medical facilities with automatic expansion
"""

import pandas as pd
import time
import json
import os
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from urllib.parse import quote
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from selenium.webdriver.chrome.options import Options

# Import frame switching utilities (assuming you have this)
import sys
sys.path.insert(0, os.path.dirname(__file__))
from utils.frame_switch import switch_left, switch_right


# ============================================================================
# REVIEW HTML PARSER
# ============================================================================

class ReviewHTMLParser:
    """Parse review HTML into structured data"""
    
    @staticmethod
    def extract_review_images(review_elem) -> List[str]:
        """Extract image URLs from review"""
        images = []
        try:
            # Look for lazyload-wrapper containing images
            lazyload = review_elem.find('div', class_='lazyload-wrapper')
            if lazyload:
                # Find all img tags
                img_tags = lazyload.find_all('img')
                for img in img_tags:
                    src = img.get('src', '')
                    if src and not src.startswith('data:image'):  # Skip base64 images
                        images.append(src)
        except Exception as e:
            print(f"          ‚ö† Error extracting images: {e}")
        
        return images
    
    @staticmethod
    def extract_reviewer_info(review_elem) -> Dict:
        """Extract reviewer information"""
        info = {}
        try:
            # Reviewer name
            name_elem = review_elem.find('span', class_='pui__NMi-Dp')
            if name_elem:
                info['reviewer_name'] = name_elem.get_text(strip=True)
            
            # Reviewer stats (Î¶¨Î∑∞ X, ÏÇ¨ÏßÑ Y)
            stats = review_elem.find_all('span', class_='pui__WN-kAf')
            if stats:
                info['reviewer_stats'] = [s.get_text(strip=True) for s in stats]
            
            # Profile link
            profile_link = review_elem.find('a', {'data-pui-click-code': 'profile'})
            if profile_link:
                info['profile_url'] = profile_link.get('href', '')
                
        except Exception as e:
            print(f"          ‚ö† Error extracting reviewer info: {e}")
        
        return info
    
    @staticmethod
    def extract_visit_keywords(review_elem) -> List[str]:
        """Extract visit keywords (ÏòàÏïΩ ÌõÑ Ïù¥Ïö©, ÎåÄÍ∏∞ ÏãúÍ∞Ñ Îì±)"""
        keywords = []
        try:
            keyword_elems = review_elem.find_all('span', class_='pui__V8F9nN')
            for elem in keyword_elems:
                text = elem.get_text(strip=True)
                if text:
                    keywords.append(text)
        except Exception as e:
            print(f"          ‚ö† Error extracting keywords: {e}")
        
        return keywords
    
    @staticmethod
    def extract_review_text(review_elem) -> str:
        """Extract review text content"""
        try:
            # Look for review text in pui__vn15t2
            text_div = review_elem.find('div', class_='pui__vn15t2')
            if text_div:
                # Get text from anchor tag
                text_elem = text_div.find('a', {'data-pui-click-code': 'rvshowmore'})
                if text_elem:
                    # Get all text including line breaks
                    return text_elem.get_text(separator='\n', strip=True)
        except Exception as e:
            print(f"          ‚ö† Error extracting review text: {e}")
        
        return ""
    
    @staticmethod
    def extract_review_date(review_elem) -> Dict:
        """Extract visit date and review submission info"""
        dates = {}
        try:
            # Visit date
            date_elems = review_elem.find_all('span', class_='pui__gfuUIT')
            for elem in date_elems:
                text = elem.get_text(strip=True)
                if 'Î∞©Î¨∏Ïùº' in str(elem):
                    time_elem = elem.find('time')
                    if time_elem:
                        dates['visit_date'] = time_elem.get_text(strip=True)
                elif 'Î≤àÏß∏ Î∞©Î¨∏' in text:
                    dates['visit_count'] = text
                elif 'Ïù∏Ï¶ù' in text:
                    dates['verification_method'] = text
        except Exception as e:
            print(f"          ‚ö† Error extracting dates: {e}")
        
        return dates
    
    @staticmethod
    def extract_owner_response(review_elem) -> Optional[Dict]:
        """Extract owner's response if exists"""
        try:
            response_div = review_elem.find('div', class_='pui__GbW8H7')
            if response_div:
                response = {}
                
                # Owner name
                owner_name = response_div.find('span', class_='pui__XE54q7')
                if owner_name:
                    response['owner_name'] = owner_name.get_text(strip=True)
                
                # Response date
                response_date = response_div.find('span', class_='pui__4APmFd')
                if response_date:
                    time_elem = response_date.find('time')
                    if time_elem:
                        response['response_date'] = time_elem.get_text(strip=True)
                
                # Response text
                text_div = response_div.find('div', class_='pui__J0tczd')
                if text_div:
                    # Try to get from anchor or span
                    text_elem = text_div.find(['a', 'span'], {'data-pui-click-code': 'text'})
                    if text_elem:
                        response['response_text'] = text_elem.get_text(separator='\n', strip=True)
                    else:
                        # Fallback to getting all text
                        response['response_text'] = text_div.get_text(separator='\n', strip=True)
                
                return response
        except Exception as e:
            print(f"          ‚ö† Error extracting owner response: {e}")
        
        return None
    
    @staticmethod
    def extract_reactions(review_elem) -> Dict:
        """Extract reactions/likes count"""
        reactions = {}
        try:
            # Look for reaction counts
            reaction_div = review_elem.find('div', class_='pui__l8k0-f')
            if reaction_div:
                count_elem = reaction_div.find('em', class_='pui__x-pa-u')
                if count_elem:
                    reactions['reaction_count'] = count_elem.get_text(strip=True)
        except Exception as e:
            print(f"          ‚ö† Error extracting reactions: {e}")
        
        return reactions
    
    @staticmethod
    def parse_single_review(review_elem) -> Dict:
        """Parse a single review element into structured data"""
        review_data = {
            'scraped_at': datetime.now().isoformat()
        }
        
        # Extract all components
        review_data['reviewer_info'] = ReviewHTMLParser.extract_reviewer_info(review_elem)
        review_data['review_text'] = ReviewHTMLParser.extract_review_text(review_elem)
        review_data['visit_info'] = ReviewHTMLParser.extract_review_date(review_elem)
        review_data['visit_keywords'] = ReviewHTMLParser.extract_visit_keywords(review_elem)
        review_data['images'] = ReviewHTMLParser.extract_review_images(review_elem)
        review_data['owner_response'] = ReviewHTMLParser.extract_owner_response(review_elem)
        review_data['reactions'] = ReviewHTMLParser.extract_reactions(review_elem)
        
        return review_data
    
    @staticmethod
    def parse_review_list(html_content: str) -> List[Dict]:
        """Parse entire review list HTML"""
        reviews = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Find review list
            review_list = soup.find('ul', id='_review_list')
            if not review_list:
                print("          ‚ö† No review list found")
                return reviews
            
            # Find all review items
            review_items = review_list.find_all('li', class_='place_apply_pui')
            
            print(f"          ‚úì Found {len(review_items)} reviews in HTML")
            
            for idx, review_elem in enumerate(review_items, 1):
                try:
                    review_data = ReviewHTMLParser.parse_single_review(review_elem)
                    review_data['review_index'] = idx
                    reviews.append(review_data)
                except Exception as e:
                    print(f"          ‚ö† Error parsing review {idx}: {e}")
            
        except Exception as e:
            print(f"          ‚úó Error parsing review list: {e}")
        
        return reviews


# ============================================================================
# REVIEW SCRAPER
# ============================================================================

class NaverMapsReviewScraper:
    """Scrape reviews from Naver Maps"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None
        self.wait = None
        self.parser = ReviewHTMLParser()
    
    def setup_driver(self):
        """Setup Chrome WebDriver"""
        options = Options()
        
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--window-size=1380,900')
        
        if self.headless:
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(3)
        self.wait = WebDriverWait(self.driver, 10)
    
    def close_driver(self):
        """Close the driver"""
        if self.driver:
            self.driver.quit()
    
    def click_review_tab(self) -> bool:
        """Click on the review tab"""
        try:
            print("        üîç Looking for review tab...")
            
            # Find review tab with data-index="1" and text "Î¶¨Î∑∞"
            review_tab = self.driver.find_element(
                By.CSS_SELECTOR, 
                'a[data-index="1"].tpj9w._tab-menu'
            )
            
            # Verify it contains "Î¶¨Î∑∞"
            if 'Î¶¨Î∑∞' in review_tab.text:
                print("        ‚úì Found review tab")
                review_tab.click()
                time.sleep(2)
                return True
            else:
                print("        ‚ö† Found tab but doesn't contain 'Î¶¨Î∑∞'")
                return False
            
        except NoSuchElementException:
            print("        ‚úó Review tab not found")
            return False
        except Exception as e:
            print(f"        ‚úó Error clicking review tab: {e}")
            return False
    
    def click_expand_all_reviews(self) -> int:
        """Click 'expand more' button until all reviews are loaded"""
        click_count = 0
        max_attempts = 100  # Safety limit
        
        print("        üìÇ Expanding all reviews...")
        
        for attempt in range(max_attempts):
            try:
                # Find the "ÌéºÏ≥êÏÑú ÎçîÎ≥¥Í∏∞" button
                expand_button = self.driver.find_element(
                    By.CSS_SELECTOR,
                    'a.fvwqf'
                )
                
                # Verify it contains the expand text
                if 'ÌéºÏ≥êÏÑú ÎçîÎ≥¥Í∏∞' in expand_button.text or 'ÎçîÎ≥¥Í∏∞' in expand_button.text:
                    # Scroll into view
                    self.driver.execute_script(
                        "arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});",
                        expand_button
                    )
                    time.sleep(0.5)
                    
                    # Click the button
                    try:
                        expand_button.click()
                    except:
                        # Use JavaScript click if regular click fails
                        self.driver.execute_script("arguments[0].click();", expand_button)
                    
                    click_count += 1
                    time.sleep(1)  # Wait for reviews to load
                    
                    if click_count % 10 == 0:
                        print(f"        ‚úì Clicked expand button {click_count} times")
                else:
                    # Button text changed, might be at the end
                    break
                    
            except (NoSuchElementException, StaleElementReferenceException):
                # Button no longer exists - all reviews loaded
                print(f"        ‚úì All reviews expanded ({click_count} clicks)")
                break
            except Exception as e:
                print(f"        ‚ö† Error during expansion: {e}")
                break
        
        if click_count >= max_attempts:
            print(f"        ‚ö† Reached maximum attempts ({max_attempts})")
        
        return click_count
    
    def extract_review_list_html(self) -> Optional[str]:
        """Extract the review list HTML"""
        try:
            # Find the review list element
            review_list = self.driver.find_element(By.ID, '_review_list')
            
            # Get outer HTML to include the ul element itself
            html_content = review_list.get_attribute('outerHTML')
            
            return html_content
            
        except NoSuchElementException:
            print("        ‚úó Review list element not found")
            return None
        except Exception as e:
            print(f"        ‚úó Error extracting review HTML: {e}")
            return None
    
    def search_and_click_first_result(self, facility_name: str) -> bool:
        """
        Search for facility by name and click first result
        Handles click interception from rapidly loading entryIframe
        """
        try:
            # Construct search URL
            encoded_name = quote(facility_name)
            search_url = f"https://map.naver.com/p/search/{encoded_name}"
            
            print(f"        üîç Searching: {search_url}")
            self.driver.get(search_url)
            time.sleep(3)
            
            # Check if entryIframe already exists (auto-navigated to single result)
            try:
                self.driver.switch_to.default_content()
                self.driver.find_element(By.ID, "entryIframe")
                print(f"        ‚úì Auto-navigated to detail page")
                return True
            except NoSuchElementException:
                pass
            
            # Switch to search results frame (left frame)
            try:
                switch_left(self.driver)
                print(f"        ‚úì Switched to searchIframe")
            except Exception as e:
                print(f"        ‚úó Could not switch to searchIframe: {e}")
                return False
            
            # Wait for results to load
            time.sleep(2)
            
            # Check if scroll container exists (multiple results case)
            try:
                scroll_container = self.driver.find_element(By.ID, "_pcmap_list_scroll_container")
                ul_element = scroll_container.find_element(By.TAG_NAME, "ul")
                li_elements = ul_element.find_elements(By.TAG_NAME, "li")
                
                if not li_elements:
                    print(f"        ‚úó No search results found")
                    return False
                
                # Get first valid li element
                first_li = None
                for li in li_elements:
                    if li.text.strip() or li.find_elements(By.TAG_NAME, "a"):
                        first_li = li
                        break
                
                if not first_li:
                    print(f"        ‚úó No valid results found")
                    return False
                
                # Find clickable link in first result
                link = None
                selectors = ["a.tzwk0", "a.place_bluelink", "a[href]"]
                
                for selector in selectors:
                    try:
                        link = first_li.find_element(By.CSS_SELECTOR, selector)
                        if link.is_displayed():
                            break
                    except:
                        continue
                
                if not link:
                    # Try any link
                    links = first_li.find_elements(By.TAG_NAME, "a")
                    for l in links:
                        if l.is_displayed():
                            link = l
                            break
                
                if not link:
                    print(f"        ‚úó No clickable link found in first result")
                    return False
                
                # Click the first result - use JavaScript if regular click fails
                print(f"        üñ±Ô∏è  Clicking first result...")
                try:
                    link.click()
                except Exception as click_error:
                    # Click intercepted - try JavaScript click
                    print(f"        ‚ö† Regular click intercepted, using JavaScript...")
                    self.driver.execute_script("arguments[0].click();", link)
                
                time.sleep(2)
                
                # Verify entryIframe appeared (even if click gave error)
                try:
                    self.driver.switch_to.default_content()
                    self.driver.find_element(By.ID, "entryIframe")
                    print(f"        ‚úì Detail page loaded")
                    return True
                except NoSuchElementException:
                    print(f"        ‚ö† entryIframe did not appear")
                    return False
                
            except NoSuchElementException:
                # No scroll container - might be single result case
                print(f"        ‚ö† No scroll container found")
                
                # Check if detail page already loaded in searchIframe
                try:
                    self.driver.find_element(By.CSS_SELECTOR, "div.place_section")
                    print(f"        ‚úì Detail content found in searchIframe")
                    return True
                except:
                    pass
                
                # Check if entryIframe appeared anyway
                try:
                    self.driver.switch_to.default_content()
                    self.driver.find_element(By.ID, "entryIframe")
                    print(f"        ‚úì entryIframe appeared")
                    return True
                except:
                    print(f"        ‚úó No results or detail page found")
                    return False
            
        except Exception as e:
            print(f"        ‚úó Error in search: {e}")
            return False
    
    def scrape_reviews_for_facility(self, facility_name: str, place_id: str) -> Dict:
        """Scrape all reviews for a single facility"""
        result = {
            'has_reviews': False,
            'review_count': 0,
            'reviews': [],
            'review_html': None,
            'scrape_error': None,
            'scraped_at': datetime.now().isoformat()
        }
        
        try:
            print(f"      üîç Scraping reviews for: {facility_name}")
            
            # Search for facility and click first result
            if not self.search_and_click_first_result(facility_name):
                result['scrape_error'] = "Could not find or click search result"
                return result
            
            # Switch to detail frame
            try:
                switch_right(self.driver)
                print("        ‚úì Switched to entryIframe")
            except Exception as e:
                print(f"        ‚ö† Could not switch to detail frame: {e}")
                result['scrape_error'] = f"Frame switch error: {e}"
                return result
            
            # Wait for page to load
            try:
                self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div.place_section'))
                )
                time.sleep(1)
            except TimeoutException:
                print("        ‚ö† Timeout waiting for page to load")
                result['scrape_error'] = "Page load timeout"
                return result
            
            # Click review tab
            if not self.click_review_tab():
                result['scrape_error'] = "Could not click review tab"
                return result
            
            # Wait for reviews to load
            time.sleep(2)
            
            # Expand all reviews
            expand_clicks = self.click_expand_all_reviews()
            print(f"        ‚úì Expanded reviews with {expand_clicks} clicks")
            
            # Extra wait for final load
            time.sleep(1)
            
            # Extract review HTML
            review_html = self.extract_review_list_html()
            
            if not review_html:
                result['scrape_error'] = "Could not extract review HTML"
                return result
            
            result['review_html'] = review_html
            
            # Parse reviews
            print("        ‚öôÔ∏è  Parsing reviews...")
            reviews = self.parser.parse_review_list(review_html)
            
            if reviews:
                result['has_reviews'] = True
                result['review_count'] = len(reviews)
                result['reviews'] = reviews
                print(f"        ‚úì Successfully parsed {len(reviews)} reviews")
            else:
                print("        ‚ö† No reviews found or parsing failed")
            
            return result
            
        except Exception as e:
            print(f"        ‚úó Error scraping reviews: {e}")
            result['scrape_error'] = str(e)
            return result


# ============================================================================
# CHECKPOINT MANAGER
# ============================================================================

class ReviewCheckpointManager:
    """Manage review scraping progress using JSON file"""
    
    def __init__(self, checkpoint_file="./data/review_scraping_progress.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
        self.progress_data = {}
        
        if self.checkpoint_file.exists():
            self.load_progress()
    
    def load_progress(self):
        """Load existing progress from JSON"""
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                self.progress_data = json.load(f)
            print(f"‚úì Loaded existing progress: {len(self.progress_data)} facilities")
        except Exception as e:
            print(f"‚ö† Could not load progress file: {e}")
            self.progress_data = {}
    
    def save_progress(self):
        """Save current progress to JSON"""
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚úó Error saving progress: {e}")
    
    def is_processed(self, place_id: str) -> bool:
        """Check if a place_id has been processed"""
        return place_id in self.progress_data
    
    def add_facility(self, place_id: str, review_data: Dict):
        """Add facility review data to progress"""
        self.progress_data[place_id] = review_data
    
    def get_stats(self) -> Dict:
        """Get statistics about current progress"""
        total = len(self.progress_data)
        with_reviews = sum(1 for v in self.progress_data.values() if v.get('has_reviews'))
        total_reviews = sum(v.get('review_count', 0) for v in self.progress_data.values())
        
        return {
            'total_processed': total,
            'with_reviews': with_reviews,
            'total_reviews_scraped': total_reviews
        }


# ============================================================================
# MAIN ORCHESTRATOR
# ============================================================================

class ReviewScrapingOrchestrator:
    """Orchestrate the review scraping process"""
    
    def __init__(self, output_dir="./data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.checkpoint_mgr = ReviewCheckpointManager(
            checkpoint_file=self.output_dir / "review_scraping_progress.json"
        )
    
    def scrape_all_reviews(self,
                           facilities_df: pd.DataFrame,
                           save_freq: int = 5,
                           headless: bool = True) -> Dict:
        """Scrape reviews for all facilities"""
        
        scraper = NaverMapsReviewScraper(headless=headless)
        scraper.setup_driver()
        
        stats = self.checkpoint_mgr.get_stats()
        total_facilities = len(facilities_df)
        already_processed = stats['total_processed']
        
        print(f"\n{'='*70}")
        print(f"STARTING REVIEW SCRAPING")
        print(f"{'='*70}")
        print(f"Total facilities: {total_facilities:,}")
        print(f"Already processed: {already_processed:,}")
        print(f"Remaining: {total_facilities - already_processed:,}")
        print(f"Save frequency: every {save_freq} facilities")
        print(f"{'='*70}\n")
        
        processed_count = 0
        
        try:
            for idx, row in facilities_df.iterrows():
                place_id = str(row['place_id'])
                facility_name = row.get('name', 'Unknown')
                
                # Skip if already processed
                if self.checkpoint_mgr.is_processed(place_id):
                    continue
                
                processed_count += 1
                current_total = already_processed + processed_count
                
                print(f"[{current_total}/{total_facilities}] {facility_name}")
                print(f"  Place ID: {place_id}")
                
                try:
                    # Scrape reviews (search is handled inside this method now)
                    review_data = scraper.scrape_reviews_for_facility(facility_name, place_id)
                    
                    # Add to checkpoint
                    self.checkpoint_mgr.add_facility(place_id, review_data)
                    
                    if review_data['has_reviews']:
                        print(f"  ‚úì Scraped {review_data['review_count']} reviews")
                    else:
                        if review_data.get('scrape_error'):
                            print(f"  ‚ö† Error: {review_data['scrape_error']}")
                        else:
                            print(f"  ‚Ñπ No reviews found")
                    
                except Exception as e:
                    print(f"  ‚úó Failed: {e}")
                    self.checkpoint_mgr.add_facility(place_id, {
                        'has_reviews': False,
                        'review_count': 0,
                        'reviews': [],
                        'review_html': None,
                        'scrape_error': str(e),
                        'scraped_at': datetime.now().isoformat()
                    })
                
                # Save progress periodically
                if processed_count % save_freq == 0:
                    self.checkpoint_mgr.save_progress()
                    stats = self.checkpoint_mgr.get_stats()
                    print(f"  üíæ Progress saved: {stats['total_processed']:,} facilities, {stats['total_reviews_scraped']:,} total reviews")
                
                time.sleep(2)  # Polite delay
            
        finally:
            scraper.close_driver()
            self.checkpoint_mgr.save_progress()
        
        return self.checkpoint_mgr.progress_data
    
    def create_review_dataset(self, facilities_df: pd.DataFrame) -> pd.DataFrame:
        """Create flat dataset with review data"""
        records = []
        
        for place_id, review_data in self.checkpoint_mgr.progress_data.items():
            # Get facility info
            facility = facilities_df[facilities_df['place_id'].astype(str) == place_id]
            
            if len(facility) > 0:
                facility_name = facility.iloc[0]['name']
            else:
                facility_name = "Unknown"
            
            if review_data.get('has_reviews') and review_data.get('reviews'):
                # Create a record for each review
                for review in review_data['reviews']:
                    record = {
                        'place_id': place_id,
                        'facility_name': facility_name,
                        'review_index': review.get('review_index'),
                        'reviewer_name': review.get('reviewer_info', {}).get('reviewer_name'),
                        'review_text': review.get('review_text'),
                        'visit_date': review.get('visit_info', {}).get('visit_date'),
                        'visit_count': review.get('visit_info', {}).get('visit_count'),
                        'verification_method': review.get('visit_info', {}).get('verification_method'),
                        'visit_keywords': json.dumps(review.get('visit_keywords', []), ensure_ascii=False),
                        'image_urls': json.dumps(review.get('images', []), ensure_ascii=False),
                        'image_count': len(review.get('images', [])),
                        'has_owner_response': review.get('owner_response') is not None,
                        'owner_response_text': review.get('owner_response', {}).get('response_text') if review.get('owner_response') else None,
                        'reaction_count': review.get('reactions', {}).get('reaction_count'),
                        'scraped_at': review.get('scraped_at')
                    }
                    records.append(record)
            else:
                # Create a single record for facilities with no reviews
                record = {
                    'place_id': place_id,
                    'facility_name': facility_name,
                    'review_index': None,
                    'reviewer_name': None,
                    'review_text': None,
                    'visit_date': None,
                    'visit_count': None,
                    'verification_method': None,
                    'visit_keywords': None,
                    'image_urls': None,
                    'image_count': 0,
                    'has_owner_response': False,
                    'owner_response_text': None,
                    'reaction_count': None,
                    'scraped_at': review_data.get('scraped_at')
                }
                records.append(record)
        
        return pd.DataFrame(records)
    
    def print_summary(self):
        """Print summary statistics"""
        stats = self.checkpoint_mgr.get_stats()
        
        print(f"\n{'='*70}")
        print(f"REVIEW SCRAPING SUMMARY")
        print(f"{'='*70}")
        print(f"Total facilities processed: {stats['total_processed']:,}")
        print(f"Facilities with reviews: {stats['with_reviews']:,}")
        print(f"Total reviews scraped: {stats['total_reviews_scraped']:,}")
        
        if stats['with_reviews'] > 0:
            avg_reviews = stats['total_reviews_scraped'] / stats['with_reviews']
            print(f"Average reviews per facility: {avg_reviews:.1f}")
        
        print(f"{'='*70}")


# ============================================================================
# DATASET LOADING
# ============================================================================

def load_facilities_dataset(source: str = "local") -> pd.DataFrame:
    """
    Load facilities dataset from local file or HuggingFace
    
    Args:
        source: "local" or "huggingface"
    """
    
    if source == "local":
        # Try multiple formats: CSV (most compatible), Parquet, Pickle
        facilities_file_csv = Path("./data/seoul_medical_facilities.csv")
        facilities_file_parquet = Path("./data/seoul_medical_facilities.parquet")
        facilities_file_pickle = Path("./data/seoul_medical_facilities.pkl")
        
        print("="*70)
        print("LOADING FACILITIES DATASET (LOCAL)")
        print("="*70)
        
        # Try CSV first (most compatible, no PyArrow needed)
        if facilities_file_csv.exists():
            try:
                facilities_df = pd.read_csv(facilities_file_csv)
                print(f"‚úì Loaded {len(facilities_df):,} facilities from CSV")
                return facilities_df
            except Exception as e:
                print(f"‚ö† Could not load CSV: {e}")
        
        # Try parquet
        if facilities_file_parquet.exists():
            try:
                facilities_df = pd.read_parquet(facilities_file_parquet)
                print(f"‚úì Loaded {len(facilities_df):,} facilities from parquet")
                return facilities_df
            except Exception as e:
                print(f"‚ö† Could not load parquet: {e}")
        
        # Try pickle
        if facilities_file_pickle.exists():
            try:
                facilities_df = pd.read_pickle(facilities_file_pickle)
                print(f"‚úì Loaded {len(facilities_df):,} facilities from pickle")
                return facilities_df
            except Exception as e:
                print(f"‚ö† Could not load pickle: {e}")
        
        # No local file found
        print(f"‚ö† No local cache found")
        print(f"  Switching to HuggingFace download...")
        return load_facilities_dataset(source="huggingface")
    
    elif source == "huggingface":
        print("="*70)
        print("LOADING FACILITIES DATASET (HUGGINGFACE)")
        print("="*70)
        
        try:
            from datasets import load_dataset
        except (ImportError, ValueError) as e:
            print(f"‚úó Cannot import 'datasets' library: {e}")
            print(f"\n" + "="*70)
            print("MANUAL DOWNLOAD REQUIRED")
            print("="*70)
            print(f"\nYour PyArrow installation is broken. Please either:")
            print(f"\n1. Fix PyArrow (recommended):")
            print(f"   pip uninstall pyarrow")
            print(f"   pip install pyarrow --break-system-packages")
            print(f"\n2. OR download dataset manually:")
            print(f"   Visit: https://huggingface.co/datasets/ValerianFourel/seoul-medical-facilities")
            print(f"   Download as CSV")
            print(f"   Save to: ./data/seoul_medical_facilities.csv")
            print(f"\nThen run the script again.")
            raise RuntimeError("Cannot load dataset - PyArrow error. See instructions above.")
        
        print("üì• Downloading from HuggingFace: ValerianFourel/seoul-medical-facilities")
        
        try:
            dataset = load_dataset("ValerianFourel/seoul-medical-facilities")
            facilities_df = dataset['train'].to_pandas()
        except Exception as e:
            print(f"‚úó Download failed: {e}")
            print(f"\nPlease download manually:")
            print(f"1. Visit: https://huggingface.co/datasets/ValerianFourel/seoul-medical-facilities")
            print(f"2. Download as CSV")
            print(f"3. Save to: ./data/seoul_medical_facilities.csv")
            raise
        
        print(f"‚úì Downloaded {len(facilities_df):,} facilities")
        
        # Save to local cache (CSV for maximum compatibility)
        cache_file_csv = Path("./data/seoul_medical_facilities.csv")
        cache_file_csv.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            facilities_df.to_csv(cache_file_csv, index=False, encoding='utf-8-sig')
            print(f"‚úì Cached to: {cache_file_csv}")
        except Exception as e:
            print(f"‚ö† Could not save cache: {e}")
        
        return facilities_df
    
    else:
        raise ValueError(f"Invalid source: {source}. Use 'local' or 'huggingface'")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution function"""
    
    print("="*70)
    print("STEP 1: LOADING FACILITIES DATASET")
    print("="*70)
    
    # Try local first, fallback to HuggingFace
    facilities_df = load_facilities_dataset(source="local")
    
    # Clean and validate
    facilities_df = facilities_df[
        facilities_df['place_id'].notna() & 
        facilities_df['name'].notna()
    ].copy()
    
    print(f"‚úì {len(facilities_df):,} facilities with valid place_id and name")
    
    # Filter for hospitals/clinics if needed
    if 'name' in facilities_df.columns:
        medical_facilities = facilities_df[
            facilities_df['name'].str.contains('Î≥ëÏõê|ÏùòÏõê', na=False)
        ]
        print(f"‚úì Filtered to {len(medical_facilities):,} medical facilities")
    else:
        medical_facilities = facilities_df
    
    print("\n" + "="*70)
    print("STEP 2: SCRAPING REVIEWS")
    print("="*70)
    
    orchestrator = ReviewScrapingOrchestrator(output_dir="./data")
    
    # Scrape reviews
    progress_data = orchestrator.scrape_all_reviews(
        medical_facilities,
        save_freq=5,
        headless=True
    )
    
    print("\n" + "="*70)
    print("STEP 3: REVIEW SCRAPING SUMMARY")
    print("="*70)
    
    orchestrator.print_summary()
    
    print("\n" + "="*70)
    print("STEP 4: CREATING REVIEW DATASET")
    print("="*70)
    
    review_df = orchestrator.create_review_dataset(medical_facilities)
    
    output_file = Path("./data/seoul_medical_reviews.parquet")
    review_df.to_parquet(output_file, index=False)
    print(f"‚úì Saved review dataset: {output_file}")
    print(f"  Total review records: {len(review_df):,}")
    
    # Also save as CSV for easy viewing
    csv_file = Path("./data/seoul_medical_reviews.csv")
    review_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"‚úì Saved CSV version: {csv_file}")
    
    print("\n" + "="*70)
    print("‚úÖ REVIEW SCRAPING COMPLETE!")
    print("="*70)
    
    return review_df


if __name__ == "__main__":
    review_df = main()